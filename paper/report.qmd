---
title: "Housing in Ames"
author: "Yanzun Jiang, Siyuan Lu, Yi Tang"
date: today
date-format: long
thanks: "Code and data supporting this proposal is available at: <https://github.com/Stary54264/Housing-in-Ames>"
format: pdf
number-sections: true
toc: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(knitr)
library(patchwork)
library(ggplot2)
library(MASS)
library(lmtest)
library(dplyr)
```

\newpage

# Introduction

The primary research question we aim to answer is: What are the key factors that significantly influence house prices in Ames from 2006 to 2010? Sale price of the house is the response variable. Predictor variables include area, overall quality index, year of construction, house facilities, and value of miscellaneous feature. The hypothesis we state is that there is a statistically significant linear relationship between certain property characteristics (predictors) and the sale price of houses (response). This hypothesis will be tested using linear regression, which would be appropriate in this case. We use residual plots and a Q-Q plot to check the assumption.

Linear regression provides coefficients that quantify the relationship between each predictor and the response variable, making it easier to interpret the impact of each factor on house prices: estimates of how much the sale price is expected to change with a one-unit change in each predictor variable, holding other variables constant. Our primary goal is to understand the impact of each predictor on house prices, so the focus should be on interpretability instead of precision or accuracy.

We found several peer-reviewed articles that focus on similar problems with this paper. "Influencing Factors Analysis of House Prices Based on Multiple Linear Regression" concludes that housing prices are negatively correlated with housing completion costs, land acquisition prices, urban residentsâ€™ disposable income, and urban population density (@influencing). This article provides some characteristics other than what we use that can also influence house price in national scope.

In "Flood Risk as a Price-Setting Factor in the Market Value of Real Property", the analyzed market consider flood risk almost indifferent with house price compare to other factors in the analysis. (@flood). This article offers a great example of our research since it uses multiple linear regression with some similar predictors as ours.

The research "House Price Prediction Using Hedonic Pricing Model and Machine Learning Techniques" shows that XGBoosting has higher accuracy in comparison to hedonic pricing model in prediction of property price. (@MLT). This article provides an alternative method to study the relationship between multiple factors and house price.

# Data Description {#sec-data}

```{r}
#| include: false
#| warning: false
#| message: false

# Read data from cleaned data file
data <- read_csv(here::here("data", "cleaned_data.csv"))
```

```{r}
#| include: false
#| warning: false
#| message: false

# Separate the dataset into two parts for better visualization
data1 <- data %>% 
  dplyr::select(sale_price, lot_area, overall_qual, year_built, roof_style)

data2 <- data %>% 
  dplyr::select(mas_vnr_area, total_bsmt_sf, central_air, garage_area, misc_val)


```

```{r}
#| label: tbl-data-1
#| tbl-cap: Preview of Data (First Half)
#| echo: false
#| warning: false
#| message: false

# Visualize the first part of the dataset
kable(head(data1), format = "markdown")
```

```{r}
#| label: tbl-data-2
#| tbl-cap: Preview of Data (Second Half)
#| echo: false
#| warning: false
#| message: false

# Visualize the second part of the dataset
kable(head(data2), format = "markdown")
```

The Ames Housing dataset (@tbl-data-1, @tbl-data-2) was sourced from the `AmesHousing` package (@ameshousing) in R (@citeR). It was originally compiled by the Ames City Assessor's Office through a comprehensive data dump of property tax records from 2006 to 2010, and it aimed to document residential property sales (@ames). The dataset was initially designed for property tax assessments and general valuation, focusing on property characteristics such as lot area, the year built, and sale price. In contrast, this research aims to analyze how various property features influence house prices in Ames.

The dataset consists of 2930 observations and 82 variables relevant to understanding housing market dynamics. After cleaning, we selected 1 response variable, `sale_price`, and 9 predictor variables: `lot_area`, `overall_qual`, `year_built`, `roof_style`, `mas_vnr_area`, `total_bsmt_sf`, `central_air`, `garage_area`, and `misc_val`.

- `sale_price`: Price of the house

- `lot_area`: Lot size

- `overall_qual`: Rates the overall material and finish of the house

- `year_built`: Original construction date

- `roof_style`: Type of roof

- `mas_vnr_area`: Masonry veneer area

- `total_bsmt_sf`: Total area of basement

- `central_air`: Central air conditioning

- `garage_area`: Size of garage

- `misc_val`: Value of miscellaneous feature

These predictor variables all show the quality of the house, which affects the price of the house directly. So, we believe there is a linear relationship between these predictor variables and the response variable.

In this analysis, we will use these packages in R: `tidyverse` (@tidyverse), `knitr` (@knitr), and `patchwork` (@patchwork).

```{r}
#| include: false
#| warning: false
#| message: false

# Generate a dataset that only consists numerical data
data_numerical <- data |>
  select(sale_price, lot_area, overall_qual, year_built,
         mas_vnr_area, total_bsmt_sf, garage_area, misc_val
  )
```

```{r}
#| label: tbl-summary
#| tbl-cap: Summarize Table of Numerical Data
#| echo: false
#| warning: false
#| message: false

# Generate a summarize table
summary_table <- data.frame(
  Mean = round(apply(data_numerical, 2, mean), 2),
  Standard_Deviation = round(apply(data_numerical, 2, sd), 2),
  Median = round(apply(data_numerical, 2, median), 2)
)

kable(summary_table, format = "markdown")
```

From the summary table (@tbl-summary), we can see that `mas_vnr_area` and `misc_val` might be right-skewed since their mean is a lot greater than their median. An interesting point is that the standard deviation of `misc_val` is quite large, which indicate that houses in Ames might differs significantly in miscellaneous features. By analyzing these variables, we aim to provide insights into how specific property characteristics affect housing prices in Ames.

# Ethics Discussion {#sec-ethics}

Our data is collected from Ames City Assessor's Office (@ames), then we cleaned the data to only keep some necessary key factors that is highly relavant to house prices. Raw and processed versions of the data from De Cock is published on Journal of Statistics Education in 2011. More detailed information about source of data is described in @sec-data. The cleaned data we are using includes some detailed information about housing characteristics, but does not contain personal identifiers.

The analysis can provide deeper insights for stakeholders, including homeowners, potential buyers, real estate agents, and policymakers, to make better decisions about buying, selling and investing in real estate. The Ames housing dataset has gained popularity, especially in the context of academic projects and machine learning competitions. It is often considered a modern alternative to the Boston Housing dataset. The dataset is well-vetted and trusted by the data science community for its comprehensiveness and relevance.

# Preliminary Results {#sec-results}

## Method

```{r}
#| label: fig-pair
#| fig-cap: Pairwise Scatterplots of Predictors
#| echo: false
#| warning: false
#| message: false

# Convert variables to factors
data$central_air <- as.factor(data$central_air)
data$roof_style <- as.factor(data$roof_style)

# Fit the linear model
model1 <- lm(sale_price ~
              lot_area + overall_qual + year_built + roof_style +
              mas_vnr_area + total_bsmt_sf + central_air + garage_area +
              misc_val, data = data
)

#summary(model1)

# Plot predictors against each other
pairs(data[, 2:9])
```
@fig-pair shows no significant curves, which indicates that every predictor is related to each other in no more complicated way than linearly. Model 1 can be used to analyze. Then, it's necessary to check the assumption of model 1, to see whether the violation, including constant variance, normality, linearity and uncorrelated errors, exist in our model.


```{r}
#| echo: false
#| warning: false
#| message: false

# Set up 2x2 plot layout
par(mfrow = c(2, 2))

# Take out the fitted values and residuals
fit <- fitted(model1)
res <- resid(model1)

# Create data frames
obs_res <- data.frame(obs = 1:length(res), res = res)
fit_res <- data.frame(fit = fit, res = res)

# 1. Linearity Assumption and Constant Variance assumption
# Residuals vs Fitted Values plot
plot(fit, res,
     xlab = "Fitted Values", ylab = "Residuals",
     main = "Residuals vs Fitted Values")
abline(h = 0, col = "red", lty = 2)

# 2. Normality Assumption (Q-Q Plot)
qqnorm(res, main = "Normal Q-Q Plot")
qqline(res, col = "red", lty = 2)

# 3. Uncorrelated Errors Assumption
# Residuals vs Predictor Index to check for independence
plot(1:length(res), res,
     xlab = "Predictor Index", ylab = "Residuals",
     main = "Residuals vs Predictor Index")
abline(h = 0, col = "red", lty = 2)


```

The model1 violates the linearity assumption as the residuals vs. fitted values plot shows non-random patterns, indicating an incomplete capture of the linear relationship. The normality assumption is also violated because the Q-Q plot reveals deviations from the diagonal line, especially at the tails, indicating non-normal residuals. The independence assumption is satisfied, as there is no systematic trend in the residuals vs. observation order plot, suggesting the residuals are randomly distributed. However, the constant variance assumption is violated, with residuals showing increased spread at higher fitted values, suggesting heteroskedasticity.

```{r}
# Set up 2x2 plot layout
par(mfrow = c(2, 2))

# Refit the model using transformed response
model2 <- lm(log(sale_price) ~
              lot_area + overall_qual + year_built + roof_style +
              mas_vnr_area + total_bsmt_sf + central_air + garage_area +
              misc_val, data = data)

# Take out the fitted values and residuals for the new model
fit2 <- fitted(model2)
res2 <- resid(model2)

# 1. Linearity Assumption and Constant Variance assumption
# Residuals vs Fitted Values plot
plot(fit2, res2,
     xlab = "Fitted Values", ylab = "Residuals",
     main = "Residuals vs Fitted Values (Transformed)")
abline(h = 0, col = "red", lty = 2)

# 2. Normality Assumption (Q-Q Plot)
qqnorm(res2, main = "Normal Q-Q Plot (Transformed)")
qqline(res2, col = "red", lty = 2)

# 3. Uncorrelated Errors Assumption
# Residuals vs Observation Order to check for independence
plot(1:length(res2), res2,
     xlab = "Predictor Index", ylab = "Residuals",
     main = "Residuals vs Predictor Index (Transformed)")
abline(h = 0, col = "red", lty = 2)

```
After applying the log transformation to the response variable, the model assumptions have improved considerably. The residuals vs. fitted values plot now shows a more random scatter around zero, suggesting better linearity and more consistent variance, addressing the heteroskedasticity issue. The Q-Q plot indicates that residuals align more closely with the normal distribution, with reduced deviations at the tails. Additionally, the residuals vs. observation order plot confirms that the independence assumption remains satisfied. Overall, the log transformation has successfully mitigated the violations, making the model assumptions more reliable for analysis and inference.
\newpage

\appendix

# Appendix
## Residual plot of predictors
```{r}
#| echo: false
#| warning: false
#| message: false


# Define the target variable and continuous predictors
target <- "sale_price"
continuous_predictors <- c("lot_area", "overall_qual", "year_built", 
                           "mas_vnr_area", "total_bsmt_sf", "garage_area", "misc_val")

# Loop through each continuous predictor and create residual plots
for (predictor in continuous_predictors) {
  # Fit a linear regression model
  model <- lm(data[[target]] ~ data[[predictor]], data = data)
  
  # Extract residuals and create a data frame for plotting
  residuals_data <- data.frame(
    Predictor = data[[predictor]],
    Residuals = resid(model)
  )
  
  # Create the residual plot
  plot <- ggplot(residuals_data, aes(x = Predictor, y = Residuals)) +
    geom_point(alpha = 0.7) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(title = paste("Residual Plot:", predictor),
         x = predictor,
         y = "Residuals") +
    theme_minimal()
  
  # Display the plot
  print(plot)
}

```
## Contributions {#sec-contribution}

Group contribution is available at <https://github.com/Stary54264/Housing-in-Ames/graphs/contributors>. Below is a more specific version of group contribution.

- Yanzun Jiang: Organized discussions and meetings; assigned tasks to group members; set up Github workspace for collaborating; downloaded data for setting up the linear regression model; cleaned data to make further analysis easier; introduced the dataset; made the summary table; created file for R code; made the reference list; revised group member's work; combined group member's work together.

- Siyuan Lu: Set research question and hypothesis; searched and read peer-reviewed articles; introduced the project; checked data ethics.

- Yi Tang: Built linear regression model; checked conditions for performing linear regression; checked extra conditions for performing multiple linear regression; showed the results of the linear regression model.

\newpage

# References
