---
title: "Housing in Ames"
author: "Yanzun Jiang, Siyuan Lu, Yi Tang"
date: today
date-format: long
thanks: "Code and data supporting this proposal is available at: <https://github.com/Stary54264/Housing-in-Ames>"
format: pdf
number-sections: true
toc: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(knitr)
```

\newpage

# Introduction

The primary research question we aim to answer is: What are the key factors that significantly influence house prices in Ames from 2006 to 2010? Sale price of the house is the response variable. Predictor variables include area, overall quality index, year of construction, house facilities, and value of miscellaneous feature. The hypothesis we state is that there is a statistically significant linear relationship between certain property characteristics (predictors) and the sale price of houses (response). This hypothesis will be tested using linear regression, which would be appropriate in this case. We use residual plots and a Q-Q plot to check the assumption.

Linear regression provides coefficients that quantify the relationship between each predictor and the response variable, making it easier to interpret the impact of each factor on house prices: estimates of how much the sale price is expected to change with a one-unit change in each predictor variable, holding other variables constant. Our primary goal is to understand the impact of each predictor on house prices, so the focus should be on interpretability instead of precision or accuracy.

We found several peer-reviewed articles that focus on similar problems with this paper. "Influencing Factors Analysis of House Prices Based on Multiple Linear Regression" concludes that housing prices are negatively correlated with housing completion costs, land acquisition prices, urban residents’ disposable income, and urban population density (@influencing). This article provides some characteristics other than what we use that can also influence house price in national scope.

In "Flood Risk as a Price-Setting Factor in the Market Value of Real Property", the analyzed market consider flood risk almost indifferent with house price compare to other factors in the analysis. (@flood). This article offers a great example of our research since it uses multiple linear regression with some similar predictors as ours.

The research "House Price Prediction Using Hedonic Pricing Model and Machine Learning Techniques" shows that XGBoosting has higher accuracy in comparison to hedonic pricing model in prediction of property price. (@MLT). This article provides an alternative method to study the relationship between multiple factors and house price.

# Data Description {#sec-data}

```{r}
#| include: false
#| warning: false
#| message: false

# Read data from cleaned data file
data <- read_csv(here::here("data", "cleaned_data.csv"))
```

```{r}
#| include: false
#| warning: false
#| message: false

# Separate the dataset into two parts for better visualization
data1 <- data |> select(
  sale_price, lot_area, overall_qual, year_built, roof_style
)

data2 <- data |> select(
  mas_vnr_area, total_bsmt_sf, central_air, garage_area, misc_val
)
```

```{r}
#| label: tbl-data-1
#| tbl-cap: Preview of Data (First Half)
#| echo: false
#| warning: false
#| message: false

# Visualize the first part of the dataset
kable(head(data1), format = "markdown")
```

```{r}
#| label: tbl-data-2
#| tbl-cap: Preview of Data (Second Half)
#| echo: false
#| warning: false
#| message: false

# Visualize the second part of the dataset
kable(head(data2), format = "markdown")
```

The Ames Housing dataset (@tbl-data-1, @tbl-data-2) was sourced from the `AmesHousing` package (@ameshousing) in R (@citeR). It was originally compiled by the Ames City Assessor's Office through a comprehensive data dump of property tax records from 2006 to 2010, and it aimed to document residential property sales (@ames). The dataset was initially designed for property tax assessments and general valuation, focusing on property characteristics such as lot area, the year built, and sale price. In contrast, this research aims to analyze how various property features influence house prices in Ames.

The dataset consists of 2930 observations and 82 variables relevant to understanding housing market dynamics. It was cleaned using `tidyverse` package (@tidyverse). After cleaning, we selected 1 response variable, `sale_price`, and 9 predictor variables: `lot_area`, `overall_qual`, `year_built`, `roof_style`, `mas_vnr_area`, `total_bsmt_sf`, `central_air`, `garage_area`, and `misc_val`.

- `sale_price`: Priceof the house in dollars

- `lot_area`: Lot size in square feet

- `overall_qual`: Rates the overall material and finish of the house

- `year_built`: Original construction date

- `roof_style`: Type of roof

- `mas_vnr_area`: Masonry veneer area in square feet

- `total_bsmt_sf`: Total square feet of basement area

- `central_air`: Central air conditioning

- `garage_area`: Size of garage in square feet

- `misc_val`: Value of miscellaneous feature in dollars

These predictor variables all show the quality of the house, which will affect the price of the house directly. So, we believe there is a linear relationship between these predictor variables and the response variable.

```{r}
#| echo: false
#| warning: false
#| message: false

# Generate a dataset that only consists numerical data
data_numerical <- data |>
  select(sale_price, lot_area, overall_qual, year_built,
         mas_vnr_area, total_bsmt_sf, garage_area, misc_val
  )
```

```{r}
#| label: tbl-summary
#| tbl-cap: Summarize Table of Numerical Data
#| echo: false
#| warning: false
#| message: false

# Generate a summarize table
summary_table <- data.frame(
  Mean = round(apply(data_numerical, 2, mean), 2),
  Standard_Deviation = round(apply(data_numerical, 2, sd), 2),
  Median = round(apply(data_numerical, 2, median), 2)
)

kable(summary_table, format = "markdown")
```

From the summary table (@tbl-summary), we can easily see that `mas_vnr_area` and `misc_val` might be right-skewed since their mean is a lot greater than their median. An interesting point is that the standard deviation of `misc_val` is quite large, which indicate that houses in Ames might differs significantly in miscellaneous features. By analyzing these variables, we aim to provide insights into how specific property characteristics affect housing prices in Ames, Iowa.

# Ethics Discussion {#sec-ethics}

Our data is collected from Ames City Assessor's Office (@ames), then we cleaned the data to only keep some necessary key factors that is highly relavant to house prices. Raw and processed versions of the data from De Cock is published on Journal of Statistics Education in 2011. More detailed information about source of data is described in @sec-data. The cleaned data we are using includes some detailed information about housing characteristics, but does not contain personal identifiers.

The analysis can provide deeper insights for stakeholders, including homeowners, potential buyers, real estate agents, and policymakers, to make better decisions about buying, selling and investing in real estate. The Ames housing dataset has gained popularity, especially in the context of academic projects and machine learning competitions. It is often considered a modern alternative to the Boston Housing dataset. The dataset is well-vetted and trusted by the data science community for its comprehensiveness and relevance.

# Preliminary Results {#sec-results}
## Test assumption

```{r}
#| echo: false
#| warning: false
#| message: false

# Convert variables to factors
data$central_air <- as.factor(data$central_air)
data$roof_style <- as.factor(data$roof_style)

# Fit the linear model
model <- lm(sale_price ~
              lot_area + overall_qual + year_built + roof_style +
              mas_vnr_area + total_bsmt_sf + central_air + garage_area +
              misc_val, data = data
)

# Take out the fitted values and residuals
fit <- fitted(model)
res <- resid(model)

# Create data frames
obs_res <- data.frame(obs = 1:length(res), res = res)
fit_res <- data.frame(fit = fit, res = res)
```

```{r}
#| label: fig-obs-res
#| fig-cap: Residuals vs. Observation Index
#| echo: false
#| warning: false
#| message: false

# Plot residuals against observation index
ggplot(obs_res, aes(x = obs, y = res)) +
  geom_point() +
  xlab("Predictor Index") +
  ylab("Residuals") +
  theme_minimal()
```

From @fig-obs-res, we can see that residuals appear to be randomly scattered along the x-axis without any special patterns, which means the dataset satisfies the uncorrelated errors assumption.

```{r}
#| label: fig-fit-res
#| fig-cap: Residuals vs. Fitted Values
#| echo: false
#| warning: false
#| message: false

# Plot residuals against fitted values
ggplot(fit_res, aes(x = fit, y = res)) +
  geom_point() +
  xlab("Fitted Values") +
  ylab("Residuals") +
  theme_minimal()
```

From @fig-fit-res, all points are randomly scattered alone a straight line, showing a linear relationship. Thus, it satisfies the linearity assumption. However, the graph shows a wider spread in errors with the increase of the fitted values. This means the dataset has violated the constant variance assumption. To solve this, we can use variance stabilizing transformation or box-cox transformation in the later analysis.

```{r}
#| label: fig-qq
#| fig-cap: Q-Q Plot
#| echo: false
#| warning: false
#| message: false

# Plot the Q-Q plot
qqnorm(res)
qqline(res, col = "red")
```

From @fig-qq, most of the points lies on the diagonal, which gives evidence that the distribution of the errors are normally distributed. Therefore, it satisfies the normality assumption.

```{r}
#| label: fig-fit-resp
#| fig-cap: Responses vs. Fitted Values
#| echo: false
#| warning: false
#| message: false

# Plot responses against fitted values
plot(x = fit, y = data$sale_price,
     xlab = "Fitted Values",
     ylab = "Responses")

abline(a = 0, b = 1, lty = 2)
```

From @fig-fit-resp, we can see that all points are randomly scattered alone the diagonal, which implies that the mean responses are a single function of a linear combination.

```{r}
#| label: fig-pair
#| fig-cap: Pairwise Scatterplots of Predictors
#| echo: false
#| warning: false
#| message: false

# Plot predictors against each other
pairs(data[, 2:9])
```
@fig-pair shows no significant curves, which indicates the predictors are linearly or weakly associated. Thus, every predictor is related to each other in no more complicated way than linearly.

Although the constant variance assumption is violated, we could still gain insights from the analysis. From the analysis above, we can find out that when every predictor is 0, `roof_style` is "Flat", and `central_air` is "N", the expected value of `sale_price` would be -426801.9. A one-unit increase in `lot_area`, `overall_qual`, `year_built`, `mas_vnr_area`, `total_bsmt_sf`, `garage_area`, and `misc_val` would result in the expected value of `sale_price` to incease by 1.24, 28596.94, 189.49, 53.04, 28.98, 62.75, and -6.88 respectively. With `cental_air`, the expected value of `sale_price` would decrease by 3355.89. Different `roof_type` would give different increase in the expected value of `sale_price`.

The results of this preliminary analysis are consistent with findings in the literature, which indicates that some aspects of a property, such its lot area, general quality, and garage size, have a substantial impact on its price. One noteworthy distinction is that this model's central air conditioning has a negative effect, in contrast to the majority of studies that suggest central air enhances home value. This gap might be the result of variables unique to the market or interactions with other variables; hence, more research is necessary to fully examine these links.

## Methods
```{r}

# Fit initial linear model
model <- lm(sale_price ~ total_bsmt_sf + lot_area + year_built, data = data)

# Residual diagnostics
hist(residuals(model), main = "Histogram of Residuals")
qqnorm(residuals(model)); qqline(residuals(model), col = "blue")
shapiro.test(residuals(model))

# Apply Box-Cox transformation
boxcox_result <- boxcox(model, lambda = seq(-2, 2, 0.1))
lambda_opt <- boxcox_result$x[which.max(boxcox_result$y)]  # Optimal lambda
data$sale_price_transformed <- (data$sale_price^lambda_opt - 1) / lambda_opt

# Refit model with transformed response
model_trans <- lm(sale_price_transformed ~ total_bsmt_sf + lot_area + year_built, data = data)
summary(model_trans)

```
To validate regression assumptions, residuals are tested for normality using histograms, Q-Q plots, and the Shapiro-Wilk test. Initial results show deviations from normality, confirmed by a Shapiro-Wilk p-value below 0.05. To address this, a Box-Cox transformation is applied to the response variable (sale_price), significantly improving normality and linearity in the residuals.
```{r}
# Check residual variance
plot(model_trans, which = 3)

# Apply square root transformation to stabilize variance
data$total_bsmt_sf_sqrt <- sqrt(data$total_bsmt_sf)
model_var_stable <- lm(sale_price_transformed ~ total_bsmt_sf_sqrt + lot_area + year_built, data = data)
summary(model_var_stable)

```
Residual plots reveal a fanning pattern, indicating violation of constant variance. Then, we try to apply a square root transformation to the highly skewed predictor total_bsmt_sf stabilizes variance, as confirmed by more evenly distributed residuals in diagnostic plots.


```{r}
# Check VIF for multicollinearity
vif(model_var_stable)

# Remove highly collinear predictors if necessary
model_reduced <- lm(sale_price_transformed ~ total_bsmt_sf_sqrt + lot_area, data = data)
summary(model_reduced)
```
The Variance Inflation Factor (VIF) values for the predictors—total_bsmt_sf_sqrt (1.20), lot_area (1.04), and year_built (1.16)—are all well below the common threshold of 10, indicating that multicollinearity is not a concern in this model. This suggests that the predictors are not excessively correlated with each other, allowing the model to produce stable and reliable coefficient estimates. Given the low VIF values, no predictors need to be removed, confirming that the model’s variables are suitable for inclusion and contribute independently to explaining the variability in housing prices.

```{r}
# Calculate SST, SSR, and RSS
sst <- sum((data$sale_price_transformed - mean(data$sale_price_transformed))^2)
ssr <- sum((fitted(model_reduced) - mean(data$sale_price_transformed))^2)
rss <- sum(residuals(model_reduced)^2)

cat("SST:", sst, "\nSSR:", ssr, "\nRSS:", rss)
```
The decomposition of variability in the transformed housing prices (sale_price_transformed) reveals that the total variability (SST) is 5485.487, with 2005.904 (SSR) explained by the predictors in the model and 3479.583 (RSS) left unexplained as residual variance. This indicates that the model accounts for a significant portion of the total variability, as reflected by the SSR, but the substantial RSS suggests there is still variability in housing prices not captured by the included predictors. This breakdown emphasizes the model’s effectiveness in explaining key trends.

```{r}
# Cook's Distance
cooksD <- cooks.distance(model_reduced)
plot(cooksD, main = "Cook's Distance", type = "h")
abline(h = 4/length(cooksD), col = "red")

# Leverage Points
leverage <- hatvalues(model_reduced)
plot(leverage, main = "Leverage Points", type = "h")
abline(h = 2*mean(leverage), col = "red")
```

Then for the outliers and influential points, We use Cook’s Distance and leverage to identify. Observations with high Cook’s Distance (e.g., above 4/n) are flagged as influential. Similarly, high leverage points (above 2 × mean leverage) suggest data points that significantly affect the regression line.

```{r}
# AIC and BIC
AIC(model_reduced)
BIC(model_reduced)

# All subsets selection
regsubsets_result <- regsubsets(sale_price_transformed ~ total_bsmt_sf_sqrt + lot_area + year_built, data = data, nvmax = 3)
summary(regsubsets_result)

# Plot model selection
plot(regsubsets_result, scale = "adj_r2")
```
The final model with the smallest AIC and BIC is selected. The exhaustive search algorithm evaluated all possible combinations of predictors: total_bsmt_sf_sqrt, lot_area, and year_built. Using only total_bsmt_sf_sqrt achieves an adjusted R^2 of 0.55, the highest among single-predictor models. Adding lot_area improves the adjusted R^2 slightly but with diminishing returns, suggesting that the combination of total_bsmt_sf_sqrt and lot_area is optimal. Including year_built does not significantly improve the adjusted R^2, indicating it does not add meaningful predictive power.

The AIC and BIC values for different models are:
Model 1 (total_bsmt_sf_sqrt): AIC = 8776.325, BIC = 8800.221.
Model 2 (total_bsmt_sf_sqrt + lot_area): AIC and BIC values are slightly lower than Model 1.
Model 3 (total_bsmt_sf_sqrt + lot_area + year_built): AIC and BIC values increase, penalizing the addition of an insignificant predictor.
Both AIC and BIC include worse results for model complexity, which means lower values indicate better models. In this case, Model 2 (total_bsmt_sf_sqrt + lot_area) achieves the lowest AIC and BIC values, making it the optimal choice.


\newpage

\appendix

# Appendix

## Contributions {#sec-contribution}

Group contribution is available at <https://github.com/Stary54264/Housing-in-Ames/graphs/contributors>. Below is a more specific version of group contribution.

- Yanzun Jiang: Organized discussions and meetings; assigned tasks to group members; set up Github workspace for collaborating; downloaded data for setting up the linear regression model; cleaned data to make further analysis easier; introduced the dataset; made the summary table; created file for R code; made the reference list; revised group member's work; combined group member's work together.

- Siyuan Lu: Set research question and hypothesis; searched and read peer-reviewed articles; introduced the project; checked data ethics.

- Yi Tang: Built linear regression model; checked conditions for performing linear regression; checked extra conditions for performing multiple linear regression; showed the results of the linear regression model.

\newpage

# References
